{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6599a65d",
   "metadata": {},
   "source": [
    "\n",
    "# Cost-sensitive learning to optimize business metrics\n",
    "\n",
    "Many real-world applications of machine learning aim to automate operational\n",
    "decisions with access to partial contextual information. This requires\n",
    "designing a decision policy that is optimal with respect to some given\n",
    "\"utility function\" or \"business metric\". The aim is therefore to maximize a\n",
    "gain or minimize a cost that is related to the decision taken by the system\n",
    "(for instance accept or reject a transaction) prior to the observation of a\n",
    "delayed outcome (discovering that the transaction was legitimate or\n",
    "fraudulent) modeled as a target random variable conditioned on observed\n",
    "contextual information.\n",
    "\n",
    "In this tutorial, we explore the concrete example of fraud detection in\n",
    "credit card transactions. We first describe the dataset used to train our\n",
    "predictive models and the function used to evaluate the resulting operational\n",
    "decisions. Then, we design a few machine learning based decision system of\n",
    "increasing sophistication and compare their business performance to baselines\n",
    "and oracle decisions on held out data.\n",
    "\n",
    "## The credit card dataset\n",
    "\n",
    "The dataset is available on OpenML at the [following URL](\n",
    "https://openml.org/search?type=data&sort=runs&status=active&id=1597).\n",
    "\n",
    "We have a local copy of the dataset in the `datasets` folder. Let's load the\n",
    "parquet file and check the data that we have at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explicitly import pyarrow since it is an optional dependency of pandas to\n",
    "# trigger the fetching when using JupyterLite with pyodide kernel. Note this is\n",
    "# an unnecessary (but harmless) import if you are not using JupyterLite with\n",
    "# pyodide.\n",
    "import pyarrow  # noqa: F401\n",
    "import pandas as pd\n",
    "\n",
    "credit_card = pd.read_parquet(\"../datasets/credit_card.parquet\", engine=\"pyarrow\")\n",
    "credit_card.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4bd43",
   "metadata": {},
   "source": [
    "\n",
    "The target column is the \"Class\" column. It informs us whether a transaction\n",
    "is fraudulent (class 1) or legitimate (class 0).\n",
    "\n",
    "We see a set of features that are anonymized starting with \"V\". Looking at\n",
    "the dataset description in OpenML, we learn that those features are the\n",
    "result of a PCA transformation. The only non-transformed features are the\n",
    "\"Time\" and \"Amount\" columns. The \"Time\" corresponds to the number of seconds\n",
    "elapsed between this transaction and the first transaction in the dataset.\n",
    "The \"Amount\" is the amount of the transaction.\n",
    "\n",
    "We first extract the target column from the other columns used as inputs to\n",
    "our predictive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686920d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"Class\"\n",
    "data = credit_card.drop(columns=[target_name, \"Time\"])\n",
    "target = credit_card[target_name].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75759403",
   "metadata": {},
   "source": [
    "\n",
    "The credit card fraud detection problem also has a special characteristic: the\n",
    "dataset is highly imbalanced. We can check the distribution of the target to\n",
    "confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b4045",
   "metadata": {},
   "source": [
    "\n",
    "The dataset is highly imbalanced with fraudulent transactions representing\n",
    "only 0.17% of the data. Since we are interested in training a machine\n",
    "learning model, we should also make sure that we have enough data points in\n",
    "the minority class to train the model by looking at the absolute numbers of\n",
    "transactions of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e995220",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13363758",
   "metadata": {},
   "source": [
    "\n",
    "We observe that we have around 500 data points for the minority class. This\n",
    "is on the low end of the number of data points required to train a machine\n",
    "learning model successfully.\n",
    "\n",
    "In addition of the target distribution, we check the distribution of the\n",
    "amount of the legitimate and fraudulent separately transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "amount_groupby_class = pd.concat([data[\"Amount\"], target], axis=1).groupby(\"Class\")[\n",
    "    \"Amount\"\n",
    "]\n",
    "\n",
    "_, ax = plt.subplots(ncols=2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "bins = np.linspace(0, data[\"Amount\"].max(), 30)\n",
    "for class_id, amount in amount_groupby_class:\n",
    "    ax[class_id].hist(amount, bins=bins, edgecolor=\"black\", density=True)\n",
    "    ax[class_id].set(\n",
    "        xlabel=\"Amount (â‚¬)\",\n",
    "        ylabel=\"Ratio of transactions\",\n",
    "        xscale=\"log\",\n",
    "        yscale=\"log\",\n",
    "        title=(\n",
    "            \"Distribution of the amount of \"\n",
    "            f'{\"fraudulent\" if class_id else \"legitimate\"} transactions'\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61230925",
   "metadata": {},
   "source": [
    "\n",
    "We cannot conclude a particular pattern in the distribution of the amount of\n",
    "the transactions apart from the fact that none of the fraudulent transactions\n",
    "has a very large amount. This information could be useful: if we train a\n",
    "predictive model on these data, we should consider that we do not know how\n",
    "our predictive model will behave on fraudulent transactions with large\n",
    "amounts in the future. It might be worth considering to have a specific\n",
    "treatment for those transactions.\n",
    "\n",
    "## Evaluating decisions with a business metric\n",
    "\n",
    "Now, we create the business metric that depends on the amount of each\n",
    "transaction.\n",
    "\n",
    "The gain of a legitimate transaction is quite easy to define since it is a\n",
    "commission that the operator receives. Here, we define it to be 2% of the\n",
    "amount of the transaction. Similarly, there is no gain in refusing a\n",
    "fraudulent transaction: the operator does not receive money from external\n",
    "actors or clients in this case.\n",
    "\n",
    "Defining a cost for refusing a legitimate transaction or accepting a\n",
    "fraudulent transaction is more complex. If the operator accepts a fraudulent\n",
    "transaction, it loses the amount of the transaction. There is also an extra\n",
    "cost involved that we define as the sum of several other costs: the cost of\n",
    "the fraud investigation, the cost of the customer support, and the cost\n",
    "related to brand reputation damage. Those additional costs should be\n",
    "specified by the data scientist in collaboration with the business\n",
    "stakeholders. A similar approach should be taken for the cost of refusing a\n",
    "legitimate transaction: the cost of customer support and the cost of a\n",
    "customer churning multiplied by estimated contribution of rejecting a\n",
    "legitimate transaction on churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commission received for each accepted legitimate transaction\n",
    "commission_transaction_gain = 0.02\n",
    "# Expected extra cost of accepting a fraudulent transaction\n",
    "avg_accept_fraud_cost = 20\n",
    "# Expected cost of refusing a legitimate transaction\n",
    "avg_refuse_legit_cost = 10\n",
    "\n",
    "\n",
    "def business_gain_func(y_true, y_pred, amount):\n",
    "    \"\"\"Business metric to optimize.\n",
    "\n",
    "    The terms computed in this function are expressed in terms of gain. It means that\n",
    "    the diagonal entry of the gain matrix G_00 and G_11 are positive values and the\n",
    "    other entries are negative values. The off-diagonal entries are however negative\n",
    "    values (a cost is considered as a negative gain).\n",
    "    \"\"\"\n",
    "    true_negative_g00 = (y_pred == 0) & (y_true == 0)\n",
    "    false_negative_g01 = (y_pred == 0) & (y_true == 1)\n",
    "    false_positive_g10 = (y_pred == 1) & (y_true == 0)\n",
    "    true_positive_g11 = (y_pred == 1) & (y_true == 1)\n",
    "\n",
    "    accept_legitimate = (amount[true_negative_g00] * commission_transaction_gain).sum()\n",
    "    accept_fraudulent = -(\n",
    "        amount[false_negative_g01].sum()\n",
    "        + (false_negative_g01 * avg_accept_fraud_cost).sum()\n",
    "    )\n",
    "    refuse_legitimate = -(false_positive_g10 * avg_refuse_legit_cost).sum()\n",
    "    refuse_fraudulent = (true_positive_g11 * 0).sum()\n",
    "\n",
    "    return accept_legitimate + accept_fraudulent + refuse_legitimate + refuse_fraudulent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6a063",
   "metadata": {},
   "source": [
    "\n",
    "We further wrap this metric function as a scikit-learn scorer object that\n",
    "computes the business metric given a fitted classifier and a test set. This\n",
    "scorer is handy because it can be used in meta-estimators, grid-search, and\n",
    "cross-validation.\n",
    "\n",
    "To create this scorer, we use the `sklearn.metrics.make_scorer` factory. The\n",
    "metric defined above requests the amount of each transaction. This variable\n",
    "is an additional metadata to be passed to the scorer and we need to use\n",
    "scikit-learn's metadata routing mechanism to pass this side information where\n",
    "appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "sklearn.set_config(enable_metadata_routing=True)\n",
    "business_gain_scorer = make_scorer(business_gain_func).set_score_request(amount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff40e91",
   "metadata": {},
   "source": [
    "\n",
    "So at this stage, we see that the amount of the transaction is used twice: once as a\n",
    "feature to train our predictive model and once as a metadata to compute the the\n",
    "business metric and thus the business performance of our model. When used as a\n",
    "feature, we are only required to have a column in `data` that contains the amount of\n",
    "each transaction. To use this information as metadata, we need to have an external\n",
    "variable that we can pass to the scorer or the model that internally routes\n",
    "this metadata to the scorer. So let's extract this variable as a standalone\n",
    "numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = credit_card[\"Amount\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562f9b1",
   "metadata": {},
   "source": [
    "\n",
    "## Investigate baseline policies\n",
    "\n",
    "Before to train a machine learning model, we investigate some baseline policies to\n",
    "serve as reference. Also, we prepare our dataset, to have a left-out test set to\n",
    "evaluate the performance of our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0187cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test, amount_train, amount_test = (\n",
    "    train_test_split(\n",
    "        data, target, amount, stratify=target, test_size=0.5, random_state=42\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683f040",
   "metadata": {},
   "source": [
    "\n",
    "The first baseline policy to evaluate is to check the performance of a policy that\n",
    "always accepts the transaction. We recall that class \"0\" is the legitimate class and\n",
    "class \"1\" is the fraudulent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d328872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "always_accept_policy = DummyClassifier(strategy=\"constant\", constant=0)\n",
    "always_accept_policy.fit(data_train, target_train)\n",
    "benefit = business_gain_scorer(\n",
    "    always_accept_policy, data_test, target_test, amount=amount_test\n",
    ")\n",
    "print(f\"Benefit of the 'always accept' policy: {benefit:,.2f}â‚¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b21653",
   "metadata": {},
   "source": [
    "\n",
    "A decision policy that considers all transactions as legitimate and as result\n",
    "never rejects a transaction would yield a total profit around 216,000â‚¬.\n",
    "\n",
    "We further evaluate a policy that rejects all transactions as fraudulent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "always_reject_policy = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "always_reject_policy.fit(data_train, target_train)\n",
    "benefit = business_gain_scorer(\n",
    "    always_reject_policy, data_test, target_test, amount=amount_test\n",
    ")\n",
    "print(f\"Benefit of the 'always reject' policy: {benefit:,.2f}â‚¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba548d1",
   "metadata": {},
   "source": [
    "\n",
    "Such a policy would entail a catastrophic loss: around 1,421,000â‚¬. This is\n",
    "expected since the vast majority of the transactions are legitimate and the\n",
    "policy would refuse them at a non-trivial cost and never collect any\n",
    "commission.\n",
    "\n",
    "Now, we evaluate a hypothetical oracle policy that would know exactly which\n",
    "transactions are fraudulent before making the decision to accept or reject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7fe8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "business_score = business_gain_func(\n",
    "    target_test,\n",
    "    target_test,\n",
    "    amount=amount_test,\n",
    ")\n",
    "print(f\"Benefit of oracle decisions (not reachable):  {business_score:,.2f}â‚¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df61978",
   "metadata": {},
   "source": [
    "\n",
    "This perfect model would make a profit of around 251,000â‚¬.\n",
    "\n",
    "Therefore, we conclude that a predictive model that a model which adapts the\n",
    "accept/reject decisions on a per transaction basis should ideally allow us to make a\n",
    "profit larger than the ~216,000â‚¬ and will be capped by an amount of ~251,000â‚¬ of the\n",
    "best of our constant baseline policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813ce5a",
   "metadata": {},
   "source": [
    "\n",
    "## Training predictive models\n",
    "\n",
    "### Logistic regression tuned with a proper scoring rule\n",
    "\n",
    "We start by training a logistic regression model with the default decision\n",
    "threshold at 0.5. Here we tune the hyperparameter `C` of the logistic\n",
    "regression with a proper scoring rule (the log loss) to ensure that the\n",
    "model's probabilistic predictions returned by its `predict_proba` method are\n",
    "as accurate as possible, irrespectively of the choice of the value of the\n",
    "decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_regression = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "param_grid = {\"logisticregression__C\": np.logspace(-6, 6, 13)}\n",
    "model = GridSearchCV(logistic_regression, param_grid, scoring=\"neg_log_loss\")\n",
    "model.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Benefit of logistic regression with default threshold: \"\n",
    "    f\"{business_gain_scorer(model, data_test, target_test, amount=amount_test):,.2f}â‚¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ba051",
   "metadata": {},
   "source": [
    "\n",
    "The business metric shows that our predictive model with a default decision threshold\n",
    "is already winning over the baseline in terms of profit and it would be already\n",
    "beneficial to use it to accept or reject transactions instead of accepting all\n",
    "transactions.\n",
    "\n",
    "However there is no reason to believe that this particular choice of decision\n",
    "threshold would be optimal for the problem at hand.\n",
    "\n",
    "### Tuned logistic regression with optimal decision threshold\n",
    "\n",
    "From a research paper by Charles Elkan [1], we know that the optimal decision\n",
    "threshold for a problem with a binary outcome can be computed by a\n",
    "closed-form formula given the following two assumptions:\n",
    "\n",
    "- the probabilistic classifier is well-calibrated,\n",
    "- the business metric can be decomposed as the sum of entries of a cost (or gain)\n",
    "  matrix.\n",
    "\n",
    "When defining our business metric, we have already expressed it as a gain matrix. So\n",
    "to use the approach described in [1], we only need to check the calibration of our\n",
    "model. In the previous section, we already tuned the hyperparameter of the logistic\n",
    "regression using a proper scoring rule that should help towards getting a\n",
    "well-calibrated model. As a first step, we assume that our classifier is\n",
    "well-calibrated. Later, we will add an extra calibration step to check if it improves\n",
    "the performance of our model in terms of the business metric.\n",
    "\n",
    "The optimal decision threshold proposed by Charles Elkan in [1] is defined as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1198bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def elkan_optimal_threshold(amount):\n",
    "    \"\"\"Compute the optimal threshold for a transaction of a given amount.\n",
    "\n",
    "    Here the terms are expressed as costs. Therefore, the diagonal entries of the\n",
    "    cost matrix (C_00 and C_11) are negative values and the off-diagonal values\n",
    "    are positive.\n",
    "\n",
    "    Note that we can multiply the numerator and the denominator by -1 to yield\n",
    "    the same formula for the entries of a gain matrix. The resulting threshold\n",
    "    would stay unchanged.\n",
    "    \"\"\"\n",
    "    c00 = -commission_transaction_gain * amount  # Accepting a legitimate transaction\n",
    "    c01 = amount + avg_accept_fraud_cost  # Accepting a fraudulent transaction\n",
    "    c10 = avg_refuse_legit_cost  # Refusing a legitimate transaction\n",
    "    c11 = 0  # Refusing a fraudulent transaction\n",
    "    return (c10 - c00) / (c10 - c00 + c01 - c11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de531fcc",
   "metadata": {},
   "source": [
    "\n",
    "Let's plot the distribution of the optimal thresholds for the transactions in\n",
    "the train set. In addition, we plot the optimal threshold as a function of the\n",
    "transaction amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7354f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "\n",
    "ax[0].hist(elkan_optimal_threshold(amount_train), bins=100, edgecolor=\"black\")\n",
    "ax[0].set(\n",
    "    xlabel=\"Optimal threshold\",\n",
    "    ylabel=\"Number of transactions\",\n",
    "    title=\"Optimal threshold distribution\",\n",
    ")\n",
    "\n",
    "x = np.linspace(amount_train.min(), amount_train.max(), 1_000)\n",
    "ax[1].plot(x, elkan_optimal_threshold(x))\n",
    "_ = ax[1].set(\n",
    "    xlabel=\"Amount (â‚¬)\",\n",
    "    ylabel=\"Optimal threshold\",\n",
    "    title=\"Optimal threshold in function of the transaction amount\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e568bf",
   "metadata": {},
   "source": [
    "\n",
    "We see that the optimal threshold varies from ~0.02 to ~0.33 depending on the\n",
    "amount of the transaction. Looking at the optimal threshold as a function of\n",
    "the transaction amount, we see that it decreases as the amount of the\n",
    "transaction increases. It means that the operator should reject a transaction\n",
    "with a large amount unless it is extremely confident that this a legitimate\n",
    "transaction. For lower amounts, it is optimal to take more risk and accept\n",
    "transactions with higher estimated probability of being fraudulent.\n",
    "\n",
    "As a first experiment, we define a decision policy with a constant decision\n",
    "threshold computed as the mean of the optimal thresholds computed for the\n",
    "transactions in the train set. Let's check the value of this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca70ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elkan_optimal_threshold(amount_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf540c90",
   "metadata": {},
   "source": [
    "\n",
    "Let's use this value to change the decision threshold of our logistic\n",
    "regression model and evaluate the performance of our model in terms of the\n",
    "business metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18aaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import FixedThresholdClassifier\n",
    "\n",
    "fixed_elkan_model = FixedThresholdClassifier(\n",
    "    estimator=model.best_estimator_,\n",
    "    threshold=elkan_optimal_threshold(amount_train).mean(),\n",
    ").fit(data_train, target_train)\n",
    "\n",
    "business_score = business_gain_scorer(\n",
    "    fixed_elkan_model, data_test, target_test, amount=amount_test\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Benefit of logistic regression with a fixed mean theoretical threshold: \"\n",
    "    f\"{business_score:,.2f}â‚¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b0fa9",
   "metadata": {},
   "source": [
    "\n",
    "We see that adjusting the decision threshold increases the gains compared to\n",
    "using the default 0.5 threshold of scikit-learn classifiers.\n",
    "\n",
    "Note that the formula we used to compute the threshold is only valid under\n",
    "the assumption that our model is well-calibrated, we could now check that it\n",
    "was really the case. Since the dataset is very imbalanced, our classifiers\n",
    "predicts very low probability for the fraudulent class most of the time, as a\n",
    "result use plot the calibration curve with a logarithmic scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "disp = CalibrationDisplay.from_estimator(\n",
    "    model.best_estimator_,\n",
    "    data_test,\n",
    "    target_test,\n",
    "    strategy=\"quantile\",\n",
    "    n_bins=5,\n",
    "    name=\"Tuned logistic regression\",\n",
    ")\n",
    "_ = disp.ax_.set(xlim=(1e-7, 0.03), ylim=(1e-7, 0.03), xscale=\"log\", yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79086d9c",
   "metadata": {},
   "source": [
    "\n",
    "The calibration looks good but not perfect. Let's attempt to improve it.\n",
    "Since we have little fraudulent data in our training set, we cannot aford to\n",
    "use a held out calibration set. Instead we use a nested cross-fitting\n",
    "procedure implemented in `CalibratedClassifierCV`: our original training set\n",
    "is splitted 5 times into train and calibration subsets and we train 5\n",
    "classifiers paired with 5 isotonic calibrators, one pair for each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11938e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "calibrated_estimator = CalibratedClassifierCV(\n",
    "    model.best_estimator_, method=\"isotonic\", cv=5\n",
    ").fit(data_train, target_train)\n",
    "disp = CalibrationDisplay.from_estimator(\n",
    "    calibrated_estimator, data_test, target_test, strategy=\"quantile\", n_bins=5\n",
    ")\n",
    "_ = disp.ax_.set(xlim=(1e-7, 0.03), ylim=(1e-7, 0.03), xscale=\"log\", yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21635a",
   "metadata": {},
   "source": [
    "\n",
    "The calibration curve of the resulting model does not look significantly\n",
    "better than that of the original logistic regression model.\n",
    "\n",
    "Still, let's see of the cross-fitted calibration procedure can lead to an\n",
    "improvement measured with the business metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_elkan_model = FixedThresholdClassifier(\n",
    "    estimator=calibrated_estimator,\n",
    "    threshold=elkan_optimal_threshold(amount_train).mean(),\n",
    ").fit(data_train, target_train)\n",
    "\n",
    "business_score = business_gain_scorer(\n",
    "    fixed_elkan_model, data_test, target_test, amount=amount_test\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Benefit of recalibrated logistic regression with a fixed mean theoretical \"\n",
    "    f\" threshold:  {business_score:,.2f}â‚¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f24847",
   "metadata": {},
   "source": [
    "\n",
    "It seems that this extra calibration step did improve the performance of our\n",
    "model in terms of the business metric. However, since we have few fraudulent\n",
    "case, the robustness of this improvement should better be assessed via an\n",
    "outer cross-validation instead of using a single global train test split.\n",
    "\n",
    "### Setting the decision threshold by direct business metric optimization\n",
    "\n",
    "In the previous section, we presented a method to compute the optimal decision\n",
    "threshold but it relied on the assumption that the probabilistic classifier is\n",
    "well-calibrated and that the business metric can be expressed as a cost matrix.\n",
    "\n",
    "Furthermore, the threshold computed with the closed form formula depends on\n",
    "the amount of the transaction. Since we wanted to used a fixed threshold for\n",
    "all decisions, we naively used the mean optimal threshold. This further\n",
    "breaks any optimality guarantee.\n",
    "\n",
    "To avoid relying on such assumptions, we can instead tune a single decision\n",
    "threshold by directly optimizing the average business metric. This\n",
    "optimization is done through a grid-search over the decision threshold\n",
    "involving a cross-validation. The class\n",
    ":class:`~sklearn.model_selection.TunedThresholdClassifierCV` is in charge of\n",
    "performing this optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "\n",
    "tuned_model = TunedThresholdClassifierCV(\n",
    "    estimator=model.best_estimator_,\n",
    "    scoring=business_gain_scorer,\n",
    "    thresholds=100,\n",
    "    n_jobs=2,\n",
    ")\n",
    "tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68107ec2",
   "metadata": {},
   "source": [
    "\n",
    "Since our business scorer requires the amount of each transaction, we need to pass\n",
    "this information in the `fit` method. The\n",
    ":class:`~sklearn.model_selection.TunedThresholdClassifierCV` is in charge of\n",
    "automatically dispatching this metadata to the underlying scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.fit(data_train, target_train, amount=amount_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1813a",
   "metadata": {},
   "source": [
    "\n",
    "Let's compare the decision threshold found by the model compared to our fixed global\n",
    "threshold from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.best_threshold_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288670c5",
   "metadata": {},
   "source": [
    "\n",
    "The resulting threshold value is much lower than the default of 0.5 but quite\n",
    "different from the mean optimal threshold computed from the closed-form\n",
    "formula.\n",
    "\n",
    "Now, let's check the performance of our model with the tuned decision\n",
    "threshold by computing the value of the business metric on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Benefit of logistic regression with a tuned threshold: \"\n",
    "    f\"{business_gain_scorer(tuned_model, data_test, target_test, amount=amount_test):,.2f}â‚¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f50728",
   "metadata": {},
   "source": [
    "\n",
    "We see that the obtained profit is quite similar (maybe slightly higher) to\n",
    "the profit obtained with the previous way of setting a fixed decision\n",
    "threshold.\n",
    "\n",
    "### Variable optimal threshold\n",
    "\n",
    "As we previously mentioned, the optimal threshold depends on each the amount\n",
    "of each transaction. However `FixedThresholdClassifier` does not support\n",
    "using variable thresholds.\n",
    "\n",
    "So instead let's write our own wrapper class to implement amount-dependent\n",
    "thresholds in the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VariableThresholdClassifier:\n",
    "\n",
    "    def __init__(self, classifier, variable_threshold):\n",
    "        self.classifier = classifier\n",
    "        self.variable_threshold = variable_threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, amount):\n",
    "        proba = self.classifier.predict_proba(X)[:, 1]\n",
    "        return (proba >= self.variable_threshold(amount)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a015ec2",
   "metadata": {},
   "source": [
    "\n",
    "This meta-estimator wraps a trained probabilistic classifier and computes the\n",
    "optimal threshold for each of the predictions and compare it with the\n",
    "classifier's probability predictions. We now evaluate the performance of this\n",
    "model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_score = business_gain_func(\n",
    "    target_test,\n",
    "    VariableThresholdClassifier(\n",
    "        model.best_estimator_,\n",
    "        variable_threshold=elkan_optimal_threshold,\n",
    "    ).predict(data_test, amount=amount_test),\n",
    "    amount=amount_test,\n",
    ")\n",
    "print(\n",
    "    f\"Benefit of logistic regression with optimal variable threshold: \"\n",
    "    f\"{business_score:,.2f}â‚¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fded1",
   "metadata": {},
   "source": [
    "\n",
    "We see that the profit is almost the same as the one obtained with the\n",
    "previous models Let's now try to combine variable thresholding with post-hoc\n",
    "calibration of the underlying classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_score = business_gain_func(\n",
    "    target_test,\n",
    "    VariableThresholdClassifier(\n",
    "        calibrated_estimator,\n",
    "        variable_threshold=elkan_optimal_threshold,\n",
    "    ).predict(data_test, amount=amount_test),\n",
    "    amount=amount_test,\n",
    ")\n",
    "print(\n",
    "    f\"Benefit of recalibrated logistic regression with optimal variable threshold: \"\n",
    "    f\"{business_score:,.2f}â‚¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793bdf",
   "metadata": {},
   "source": [
    "\n",
    "The resulting profit seems to be slightly better, so there is a potential\n",
    "benefit using both variable thresholding. However, the improvement is not\n",
    "should be further confirmed via an outer cross-validation.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The analysis presented in this example only uses 2 days of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d8f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "((credit_card[\"Time\"].max() - credit_card[\"Time\"].min()) / (60 * 60 * 24)).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae63b92",
   "metadata": {},
   "source": [
    "\n",
    "The difference in profit between the \"always accept\" baseline (216 kâ‚¬) and a\n",
    "logistic regression model with a default threshold at 0.5 (235 kâ‚¬) was\n",
    "already substantial: 19 kâ‚¬ in two days, which translate to around 3.5 Mâ‚¬ per\n",
    "year.\n",
    "\n",
    "When we further tune the threshold we could increase the profit to 239 kâ‚¬\n",
    "which would translate to an additional 730 kâ‚¬ increase in profit per year\n",
    "so tuning the threshold is worth the effort.\n",
    "\n",
    "Off-course those numbers should be confirmed with data collected over a\n",
    "longer period of time.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Charles Elkan, \"The Foundations of Cost-Sensitive Learning\", International joint\n",
    " conference on artificial intelligence. Vol. 17. No. 1. Lawrence Erlbaum Associates\n",
    " Ltd, 2001. [URL](https://cseweb.ucsd.edu/~elkan/rescale.pdf)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
