{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef5cdad",
   "metadata": {},
   "source": [
    "\n",
    "# Reading calibration curves\n",
    "\n",
    "We use different sets of predictions leading to various calibration curves\n",
    "with typical shapes, from which we want to derive insights into the\n",
    "underlying models that generated these predictions, namely:\n",
    "\n",
    "- a well calibrated model;\n",
    "- an overconfident model;\n",
    "- an underconfident model;\n",
    "- a model fit with improper class weights/resampling.\n",
    "\n",
    "First, let's gather different prediction sets for the same classification\n",
    "task. This is achieved using a script named `_generate_predictions.py`. This\n",
    "script stores the true labels and the predicted probability estimates of\n",
    "several models into the `predictions` folder. We don't need to understand\n",
    "what model they correspond to, we just want to analyze the calibration of\n",
    "these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef871294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have scikit-learn >= 1.5\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to the magic command \"%run _generate_predictions.py\" but it allows this\n",
    "# file to be executed as a Python script.\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"run\", \"../python_files/_generate_predictions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498782c",
   "metadata": {},
   "source": [
    "\n",
    "We first load the true testing labels of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.load(\"../predictions/y_true.npy\")\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_labels, counts = np.unique(y_true, return_counts=True)\n",
    "unique_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc579087",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672af809",
   "metadata": {},
   "source": [
    "\n",
    "We observe that we have a binary classification problem. Now, we load different\n",
    "sets of predictions of probabilities estimated by different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_1 = np.load(\"../predictions/y_prob_1.npy\")\n",
    "y_proba_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475ccfb",
   "metadata": {},
   "source": [
    "\n",
    "We assess the calibration of the model that outputs these predictions by\n",
    "plotting the calibration curve:\n",
    "\n",
    "- data points are first **grouped into bins of similar predicted\n",
    "  probabilities**;\n",
    "- then for each bin, we plot a point of the curve that represents the\n",
    "  **fraction of observed positive labels in a bin** against the **mean\n",
    "  predicted probability for the positive class in that bin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ae6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # noqa: F401\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_predictions = {\n",
    "    \"Well calibrated\": y_proba_1,\n",
    "}\n",
    "\n",
    "\n",
    "def plot_calibration_curves(y_true, model_predictions):\n",
    "    _, ax = plt.subplots()\n",
    "    for model_name, y_proba in model_predictions.items():\n",
    "        CalibrationDisplay.from_predictions(\n",
    "            y_true, y_proba, n_bins=10, strategy=\"quantile\", name=model_name, ax=ax\n",
    "        )\n",
    "    ax.axvline(0.5, color=\"gray\", linestyle=\"--\")\n",
    "    ax.set(xlim=(0, 1), ylim=(0, 1), aspect=\"equal\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "plot_calibration_curves(y_true, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69061f58",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the calibration curve is close to the diagonal that represents a\n",
    "perfectly calibrated model. It means that relying on the predicted probabilities\n",
    "will provide reliable estimates of the true probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b053c3a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "We now repeat the same analysis for the other sets of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions[\"Overconfident\"] = np.load(\"../predictions/y_prob_2.npy\")\n",
    "plot_calibration_curves(y_true, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca3ac6",
   "metadata": {},
   "source": [
    "\n",
    "Let's first focus on the **right part of the curve**, that is when the models\n",
    "predicts the positive class, assuming a decision threshold at 0.5. The\n",
    "calibration curve is below the diagonal. It means that the fraction of\n",
    "observed positive data points is lower than the predicted probabilities.\n",
    "Therefore, our model over-estimates the probabilities of the positive class\n",
    "when the predictions are higher than the default threshold: the model is\n",
    "therefore **overconfident in predicting the positive class**.\n",
    "\n",
    "Let's now focus on the **left part of the curve**, that is when the model\n",
    "predicts the negative class. The curve is above the diagonal, meaning that\n",
    "the fraction of observed positive data points is higher than the predicted\n",
    "probabilities of the positive class. This also means that the fraction of\n",
    "observed negatives is lower than the predicted probabilities of the negative\n",
    "class. Therefore, our model is also **overconfident in predicting the\n",
    "negative class**.\n",
    "\n",
    "In conclusion, our model is overconfident when predicting either classes:\n",
    "the predicted probabilities are too close to 0 or 1 compared to the observed\n",
    "fraction of positive data points in each bin.\n",
    "\n",
    "Let's use the same approach to analyze other typical calibration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07253f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions[\"Underconfident\"] = np.load(\"../predictions/y_prob_3.npy\")\n",
    "plot_calibration_curves(y_true, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429af06",
   "metadata": {},
   "source": [
    "\n",
    "Here, we observe the opposite behaviour compared to the previous case: our model\n",
    "outputs probabilities that are too close to 0.5 compared to the empirical positive\n",
    "class fraction. Therefore, this model is underconfident.\n",
    "\n",
    "Let's check the last set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8aecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions[\"Improper class weights\"] = np.load(\"../predictions/y_prob_4.npy\")\n",
    "plot_calibration_curves(y_true, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec42bd8",
   "metadata": {},
   "source": [
    "\n",
    "Here, we observe a curve that off the diagonal without ever crossing it. This\n",
    "is another typical case of mis-calibration: in this case the model always\n",
    "over estimates the true probabilities, both below and above the 0.5\n",
    "threshold. As we will explore in a later notebook, this is a typical behavior\n",
    "of a model trained with improper class weights or resampling strategies.\n",
    "\n",
    "Finally, let's also display the ROC curves computed for all those models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb8380",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for model_name, y_proba in model_predictions.items():\n",
    "    CalibrationDisplay.from_predictions(\n",
    "        y_true,\n",
    "        y_proba,\n",
    "        n_bins=10,\n",
    "        strategy=\"quantile\",\n",
    "        name=model_name,\n",
    "        ax=axs[0],\n",
    "    )\n",
    "    roc_display = RocCurveDisplay.from_predictions(\n",
    "        y_true, y_proba, name=model_name, ax=axs[1]\n",
    "    )\n",
    "\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "_ = axs[0].set(title=\"Calibration curves\", xlim=(0, 1), ylim=(0, 1), aspect=\"equal\")\n",
    "_ = axs[1].set(title=\"Receiver operator curves\", aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8734a1",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the all the ROC curves overlap exactly and as a result the\n",
    "ROC AUC values are exactly the same. This means that all those predictions\n",
    "have the same ability to discriminate between the two classes, also known as\n",
    "\"ranking power\" or \"resolution\". The models predictions only differ in their\n",
    "calibration.\n",
    "\n",
    "This highlights the fact that ROC curves and ranking metrics such as ROC AUC\n",
    "and average precision are blind to the calibration of probabilistic models.\n",
    "On the contrary, metrics such as log loss or Brier score are sensitive to\n",
    "both the calibration and the ranking power of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afef2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "model_scores = []\n",
    "for model_name, y_proba in model_predictions.items():\n",
    "    model_scores.append(\n",
    "        {\n",
    "            \"Model\": model_name,\n",
    "            \"Log-loss\": log_loss(y_true, y_proba),\n",
    "            \"Brier score\": brier_score_loss(y_true, y_proba),\n",
    "            \"ROC AUC\": roc_auc_score(y_true, y_proba),\n",
    "            \"Average Precision\": average_precision_score(y_true, y_proba),\n",
    "        }\n",
    "    )\n",
    "pd.DataFrame(model_scores).set_index(\"Model\").round(3)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
