{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53805c1e",
   "metadata": {},
   "source": [
    "\n",
    "# What is a calibration curve?\n",
    "\n",
    "Before we dive into how to interpret a calibration curve, let's start by getting\n",
    "intuitions on what it graphically represents. In this exercise, you will build your\n",
    "own calibration curve.\n",
    "\n",
    "To simplify the process, we only focus on the output of a binary classifier but\n",
    "without going into details on the model used to generate the predictions. We later\n",
    "discuss the implications of the data modeling process on the calibration curve.\n",
    "\n",
    "So let's first generate some predictions. The generative process is located in the\n",
    "file `_generate_predictions.py`. This process stores the true labels and the\n",
    "predicted probability estimates of several models into the `predictions` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179428b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have scikit-learn >= 1.5\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to the magic command \"%run _generate_predictions.py\" but it allows this\n",
    "# file to be executed as a Python script.\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"run\", \"../python_files/_generate_predictions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddcc6b9",
   "metadata": {},
   "source": [
    "\n",
    "Let's load a the true labels and the predicted probabilities of one of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.load(\"../predictions/y_true.npy\")\n",
    "y_prob = np.load(\"../predictions/y_prob_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4397625",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432835f",
   "metadata": {},
   "source": [
    "\n",
    "`y_true` contains the true labels. In this case, we face a binary classification\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a077ab0",
   "metadata": {},
   "source": [
    "\n",
    "`y_prob` contains the predicted probabilities of the positive class estimated by a\n",
    "given model.\n",
    "\n",
    "As discussed earlier, we could evaluate the discriminative power of the model using\n",
    "a metric such as the ROC-AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac760f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc_score(y_true, y_prob):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cde01",
   "metadata": {},
   "source": [
    "\n",
    "The score is much above 0.5 and it means that our model is able to discriminate\n",
    "between the two classes. However, this score does not tell us if the predicted\n",
    "probabilities are well calibrated.\n",
    "\n",
    "We can provide an example of a well-calibrated probability: for a given sample, the\n",
    "predicted probabilities is considered well-calibrated if the proportion of samples\n",
    "where this probability is predicted is equal to the probability itself.\n",
    "\n",
    "It is therefore possible to group predictions into bins based on their predicted\n",
    "probabilities and to calculate the proportion of positive samples in each bin and\n",
    "compare it to the average predicted probability in the bin.\n",
    "\n",
    "Scikit-learn provide a utility to plot this graphical representation. It is available\n",
    "through the class `sklearn.calibration.CalibrationDisplay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "n_bins = 10\n",
    "disp = CalibrationDisplay.from_predictions(\n",
    "    y_true, y_prob, n_bins=n_bins, strategy=\"uniform\"\n",
    ")\n",
    "_ = disp.ax_.set(xlim=(0, 1), ylim=(0, 1), aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9181b52",
   "metadata": {},
   "source": [
    "\n",
    "As a pedagogical exercise, we will build the calibration curve from scratch. This\n",
    "will help us understand the underlying process. The calibration curve is built by\n",
    "following these steps:\n",
    "\n",
    "1. Bin the predicted probabilities (i.e. `y_prob`) into 10 bins. You can use the\n",
    "   `pd.cut` function from the `pandas` library. It will return a `Categorical`\n",
    "   object where we get the bin identifier for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c44bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bin_identifier = pd.cut(y_prob, bins=n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb966d",
   "metadata": {},
   "source": [
    "\n",
    "2. Create a DataFrame with the true labels, predicted probabilities, and bin\n",
    "   identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_true,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"bin_identifier\": bin_identifier,\n",
    "    }\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfccf1b",
   "metadata": {},
   "source": [
    "\n",
    "3. Group the DataFrame by the bin identifier and calculate the mean of the true\n",
    "   labels and the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_predicted_probabilities = predictions.groupby(\n",
    "    \"bin_identifier\", observed=True\n",
    ").y_prob.mean()\n",
    "fraction_positive_samples = predictions.groupby(\n",
    "    \"bin_identifier\", observed=True\n",
    ").y_true.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748809c",
   "metadata": {},
   "source": [
    "\n",
    "4. Plot the calibration curve by plotting the average predicted probabilities\n",
    "   against the fraction of positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(avg_predicted_probabilities, fraction_positive_samples, \"s-\")\n",
    "ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "ax.legend()\n",
    "_ = ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    xlabel=\"Average predicted probability\",\n",
    "    ylabel=\"Fraction of positive samples\",\n",
    "    aspect=\"equal\",\n",
    "    title=\"Calibration curve\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97be4ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Scikit-learn also provides a parameter `strategy` that is the strategy to adopt to\n",
    "create the bins. By default, we used the `\"uniform\"` strategy. However, it can happen\n",
    "that only few samples are present in some bins, leading to a noisy calibration curve.\n",
    "\n",
    "The strategy `\"quantile\"` ensures that each bin has the same number of samples. We\n",
    "can show it on our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = CalibrationDisplay.from_predictions(\n",
    "    y_true, y_prob, n_bins=n_bins, strategy=\"quantile\"\n",
    ")\n",
    "_ = disp.ax_.set(xlim=(0, 1), ylim=(0, 1), aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036771b3",
   "metadata": {},
   "source": [
    "\n",
    "Modify your previous implementation such that `pd.cut` uses quantiles to create the\n",
    "bins instead of the default uniform behaviour. Specifically, look at the `bins`\n",
    "parameter and the function `np.quantile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_identifier = pd.cut(y_prob, bins=np.quantile(y_prob, np.linspace(0, 1, n_bins)))\n",
    "predictions = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_true,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"bin_identifier\": bin_identifier,\n",
    "    }\n",
    ")\n",
    "avg_predicted_probabilities = predictions.groupby(\n",
    "    \"bin_identifier\", observed=True\n",
    ").y_prob.mean()\n",
    "fraction_positive_samples = predictions.groupby(\n",
    "    \"bin_identifier\", observed=True\n",
    ").y_true.mean()\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(avg_predicted_probabilities, fraction_positive_samples, \"s-\")\n",
    "ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "ax.legend()\n",
    "_ = ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    xlabel=\"Average predicted probability\",\n",
    "    ylabel=\"Fraction of positive samples\",\n",
    "    aspect=\"equal\",\n",
    "    title=\"Calibration curve\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
