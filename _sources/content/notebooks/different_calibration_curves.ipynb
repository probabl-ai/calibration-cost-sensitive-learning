{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a652fab2",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding calibration curves\n",
    "\n",
    "In this notebook, we explore calibration curves. We use different set of predictions\n",
    "leading to different calibration curves from which we want to build insights and\n",
    "understand the impact and meaning when it comes to our predictive models.\n",
    "\n",
    "So let's first generate some predictions. The generative process is located in the\n",
    "file `_generate_predictions.py`. This process stores the true labels and the\n",
    "predicted probability estimates of several models into the `predictions` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29195898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have scikit-learn >= 1.5\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73110ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to the magic command \"%run _generate_predictions.py\" but it allows this\n",
    "# file to be executed as a Python script.\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"run\", \"../python_files/_generate_predictions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7338f3",
   "metadata": {},
   "source": [
    "\n",
    "We first load the true testing labels of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.load(\"../predictions/y_true.npy\")\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fcd72",
   "metadata": {},
   "source": [
    "\n",
    "We observed that we have a binary classification problem. Now, we load different\n",
    "sets of predictions of probabilities estimated by different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = np.load(\"../predictions/y_prob_1.npy\")\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec77ac",
   "metadata": {},
   "source": [
    "\n",
    "We assess the calibration of the model that provide these predictions by plotting\n",
    "the calibration curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # noqa: F401\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "params = {\"n_bins\": 10, \"strategy\": \"quantile\"}\n",
    "disp = CalibrationDisplay.from_predictions(y_true, y_proba, **params)\n",
    "_ = disp.ax_.set(\n",
    "    title=\"Calibrated model\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734b66c",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the calibration curve is close to the diagonal that represents a\n",
    "perfectly calibrated model. It means that relying on the predicted probabilities\n",
    "will provide reliable estimates of the true probabilities.\n",
    "\n",
    "For further analysis, we also compute the ROC AUC score of the model. It recall that\n",
    "this metric is only based on the ranking of the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc_score(y_true, y_proba):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb47b96",
   "metadata": {},
   "source": [
    "\n",
    "So in addition of being well calibrated, our model is capable of distinguishing\n",
    "between the two classes.\n",
    "\n",
    "We now repeat the same analysis for the other sets of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220afdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = np.load(\"../predictions/y_prob_2.npy\")\n",
    "disp = CalibrationDisplay.from_predictions(y_true, y_proba, **params)\n",
    "_ = disp.ax_.set(\n",
    "    title=\"Uncalibrated model\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_true, y_proba):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e920ae8",
   "metadata": {},
   "source": [
    "\n",
    "We see that the ROC AUC score is the same. It means that whatever transformation we\n",
    "apply to the predicted probabilities, it did not change the ranking of the\n",
    "predictions. However, the calibration curve is not following the diagonal anymore.\n",
    "\n",
    "Let's slightly modify the graphical representation to better interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa92ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = CalibrationDisplay.from_predictions(y_true, y_proba, **params)\n",
    "disp.ax_.axvline(0.5, color=\"tab:orange\", label=\"Estimated probability = 0.5\")\n",
    "disp.ax_.legend()\n",
    "_ = disp.ax_.set(\n",
    "    title=\"Uncalibrated model\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ea148",
   "metadata": {},
   "source": [
    "\n",
    "We added a vertical line at a 0.5 threshold for the mean predicted probability. Let's\n",
    "first focus on the left part of the curve. The calibration curve is above the diagonal\n",
    "it means that the fraction of positive samples is higher than the predicted\n",
    "probabilities. Therefore, our model is too conservative at predicting the positive\n",
    "class and is therefore under-confident. On the right part of the curve, the\n",
    "calibration curve is below the diagonal. It means that the fraction of positive\n",
    "samples is lower than the predicted probabilities. Our model is therefore\n",
    "over-confident.\n",
    "\n",
    "Let's use the same approach to analyze some other typical calibration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = np.load(\"../predictions/y_prob_3.npy\")\n",
    "disp = CalibrationDisplay.from_predictions(y_true, y_proba, **params)\n",
    "disp.ax_.axvline(0.5, color=\"tab:orange\", label=\"Estimated probability = 0.5\")\n",
    "disp.ax_.legend()\n",
    "_ = disp.ax_.set(\n",
    "    title=\"Uncalibrated model\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_true, y_proba):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73f484",
   "metadata": {},
   "source": [
    "\n",
    "As in the previous case, we observe that the transformation applied does not change\n",
    "the ranking of the predictions since we obtain the same ROC AUC score. However, we\n",
    "have a completely different calibration curve that is not following the diagonal as\n",
    "well.\n",
    "\n",
    "First, we note that the curve does not span on the left side of the plot. It means\n",
    "that our model never predicts probabilities lower than 0.5 even though there is\n",
    "a small fraction of positive samples. On the right side of the curve, we are below\n",
    "the diagonal meaning that our model is predicting higher probabilities estimates\n",
    "than the fraction of positive samples. Our model is clearly over-confident.\n",
    "\n",
    "Let's check the last set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = np.load(\"../predictions/y_prob_4.npy\")\n",
    "disp = CalibrationDisplay.from_predictions(y_true, y_proba, **params)\n",
    "disp.ax_.axvline(0.5, color=\"tab:orange\", label=\"Estimated probability = 0.5\")\n",
    "disp.ax_.legend()\n",
    "_ = disp.ax_.set(\n",
    "    title=\"Uncalibrated model\",\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_true, y_proba):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410f854",
   "metadata": {},
   "source": [
    "\n",
    "Here, we observe the opposite behaviour compared to the previous case: our model\n",
    "output relatively low probabilities while the fraction of positive samples is high.\n",
    "Therefore, this model is under-confident."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
