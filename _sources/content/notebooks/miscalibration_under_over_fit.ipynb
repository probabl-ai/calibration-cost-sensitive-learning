{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c07366c",
   "metadata": {},
   "source": [
    "\n",
    "# Miscalibration caused by inappropriate hyperparameters\n",
    "\n",
    "Model complexity is controlled both by the choice of the model class, the\n",
    "choice of preprocessing steps in the ML pipeline and by the choice of\n",
    "hyperparameters at each step. Depending on those choices, we can obtain\n",
    "pipelines that are under-fitting or over-fitting. In this notebook, we\n",
    "investigate the relationship between models hyperparameters, model\n",
    "complexity, and their calibration.\n",
    "\n",
    "Let's start by defining our classification problem: we use the so-called\n",
    "(noisy) XOR problem. The function `xor_generator` generates a dataset with\n",
    "two features and the target variable following the XOR logic. We add some\n",
    "noise to the generative process to ensure that the target is not a fully\n",
    "deterministic function of the features as this is never the case in real\n",
    "applications of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9812a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have scikit-learn >= 1.5\n",
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29141fc9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def xor_generator(n_samples=1_000, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.uniform(low=-3, high=3, size=(n_samples, 2))\n",
    "    unobserved = rng.normal(loc=0, scale=0.5, size=(n_samples, 2))\n",
    "    y = np.logical_xor(X[:, 0] + unobserved[:, 0] > 0, X[:, 1] + unobserved[:, 1] > 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf51d1b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "We can now generate a dataset and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be521d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, y_train = xor_generator(seed=0)\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(*X_train.T, c=y_train, cmap=\"coolwarm\", edgecolors=\"black\", alpha=0.5)\n",
    "_ = ax.set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    title=\"XOR problem\",\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb3f8b",
   "metadata": {},
   "source": [
    "\n",
    "The XOR problem exhibits a non-linear decision link between the features and\n",
    "the the target variable. Therefore, a linear classification model is not be\n",
    "able to separate the classes correctly. Let's confirm this intuition by\n",
    "fitting a logistic regression model to such a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2eeeaa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "Let's visualize the decision boundary learned by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feaaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "params = {\n",
    "    \"cmap\": \"coolwarm\",\n",
    "    \"response_method\": \"predict_proba\",\n",
    "    \"plot_method\": \"contourf\",\n",
    "    # make sure to have a range of 0 to 1 for the probability\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 1,\n",
    "}\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, X_train, ax=ax, **params)\n",
    "ax.scatter(*X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5)\n",
    "fig.colorbar(disp.surface_, ax=ax, label=\"Probability estimate\")\n",
    "_ = ax.set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    title=\"Soft decision boundary of a logistic regression\",\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e22fc",
   "metadata": {},
   "source": [
    "\n",
    "We see that the probability estimate is almost constant (near 0.5) everywhere\n",
    "in the feature space: the model is really uncertain.\n",
    "\n",
    "We therefore need a more expressive model to capture the non-linear\n",
    "relationship between the features and the target variable. Crafting a\n",
    "pre-processing step to transform the features into a higher-dimensional space\n",
    "could help.\n",
    "\n",
    "Here we choose to create a pipeline that includes a first spline expansion for\n",
    "each feature followed a polynomial transformation to capture multiplicative\n",
    "interaction across features before passing the result to a final logistic\n",
    "regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e914866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    # Expand each feature marginally using splines:\n",
    "    SplineTransformer(),\n",
    "    # Model multiplicative interactions across features:\n",
    "    PolynomialFeatures(interaction_only=True),\n",
    "    # Increase the number of iterations to ensure convergence even with low\n",
    "    # regularization when tuning C later.\n",
    "    LogisticRegression(max_iter=10_000),\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1949cf6",
   "metadata": {},
   "source": [
    "\n",
    "Let's check the decision boundary of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, X_train, ax=ax, **params)\n",
    "ax.scatter(*X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5)\n",
    "fig.colorbar(disp.surface_, ax=ax, label=\"Probability estimate\")\n",
    "_ = ax.set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    title=\"Soft decision boundary of a logistic regression\\n with pre-processing\",\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df05eb",
   "metadata": {},
   "source": [
    "\n",
    "We see that our refined pipeline is capable of capturing the non-linear\n",
    "relationship between the features and the target variable. The probability\n",
    "estimates are now varying across the samples.\n",
    "\n",
    "To evaluate the calibration of the model, we plot the calibration curve on an\n",
    "independent test set. Here we generate a test set with a large number of data\n",
    "points to get a stable estimate of the quality of the model. Using large test\n",
    "sets is a luxurary that we can typically not afford in practice, we only do\n",
    "it here for educational reasons. The alternative would be to run the full\n",
    "analysis multiple times via cross-validation but we refrain from doing this\n",
    "here to keep the notebook simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = xor_generator(n_samples=10_000, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "disp = CalibrationDisplay.from_estimator(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    strategy=\"quantile\",\n",
    "    n_bins=10,\n",
    ")\n",
    "_ = disp.ax_.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9062e8",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the calibration of the model is far from ideal. Is there a\n",
    "way to improve the calibration of our model?\n",
    "\n",
    "As an exercise, let's try to three different hyperparameters configurations:\n",
    "- one configuration with 5 knots (i.e. `n_knots`) for the spline\n",
    "  transformation and a regularization parameter `C` of 1e-1 for the logistic\n",
    "  regression,\n",
    "- one configuration with 7 knots for the spline transformation and a\n",
    "  regularization parameter `C` of 1e1 for the logistic regression,\n",
    "- one configuration with 15 knots for the spline transformation and a\n",
    "  regularization parameter `C` of 1e4 for the logistic regression.\n",
    "\n",
    "For each configuration, plot the decision boundary and the calibration curve.\n",
    "What can you observe in terms of under-/over-fitting and calibration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c07220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_configs = [\n",
    "    {\"splinetransformer__n_knots\": 5, \"logisticregression__C\": 1e-1},\n",
    "    {\"splinetransformer__n_knots\": 7, \"logisticregression__C\": 1e1},\n",
    "    {\"splinetransformer__n_knots\": 15, \"logisticregression__C\": 1e4},\n",
    "]\n",
    "\n",
    "# TODO: write me!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b347f28",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e24d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_configs = [\n",
    "    {\"splinetransformer__n_knots\": 5, \"logisticregression__C\": 1e-1},\n",
    "    {\"splinetransformer__n_knots\": 7, \"logisticregression__C\": 1e1},\n",
    "    {\"splinetransformer__n_knots\": 15, \"logisticregression__C\": 1e4},\n",
    "]\n",
    "\n",
    "for model_params in param_configs:\n",
    "    model.set_params(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(model, X_train, ax=ax[0], **params)\n",
    "    ax[0].scatter(\n",
    "        *X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5\n",
    "    )\n",
    "\n",
    "    ax[0].set(\n",
    "        xlim=(-3, 3),\n",
    "        ylim=(-3, 3),\n",
    "        xlabel=\"Feature 1\",\n",
    "        ylabel=\"Feature 2\",\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "\n",
    "    CalibrationDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        strategy=\"quantile\",\n",
    "        n_bins=10,\n",
    "        ax=ax[1],\n",
    "    )\n",
    "    ax[1].set(aspect=\"equal\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Number of knots: {model_params['splinetransformer__n_knots']}, \"\n",
    "        f\"Regularization 'C': {model_params['logisticregression__C']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335bc13",
   "metadata": {},
   "source": [
    "\n",
    "From the previous exercise, we observe that whether we have an under-fitting\n",
    "or over-fitting model impact its calibration. With a high regularization\n",
    "(i.e. `C=1e-1`), we see that the model undefits as it is too constrained to\n",
    "be able to predict high enough probabilties in areas of the feature space\n",
    "without any class ambiguity. It translates into obtaining a vertical-ish\n",
    "calibration curve meaning that our model is underconfident.\n",
    "\n",
    "On the other hand, if we have a low regularization (i.e. `C=1e4`), and allows\n",
    "the the model to be flexible by having a large number of knots, we see that\n",
    "the model overfits since it is able to isolate noisy samples in the feature\n",
    "space. It translates into a calibration curve where we observe that our model\n",
    "is overconfident.\n",
    "\n",
    "Finally, there is a sweet spot where the model between underfitting and\n",
    "overfitting. In this case, we also get a well calibrated model.\n",
    "\n",
    "We can push the analysis further by assessing the impact of wider range of\n",
    "hyperparameters:\n",
    "\n",
    "- varying `n_knots` of the `SplineTransformer` preprocessing step,\n",
    "- choosing whether or not to model multiplicative feature interactions using\n",
    "  a `PolynomialFeatures`,\n",
    "- varying the regularization parameter `C` of the final `LogisticRegression`\n",
    "  classifier.\n",
    "\n",
    "We can plot the full grid of hyperparameters to see the effect on the\n",
    "decision boundary and the calibration curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d12f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = list(\n",
    "    ParameterGrid(\n",
    "        {\n",
    "            \"logisticregression__C\": np.logspace(-1, 3, 5),\n",
    "            \"splinetransformer__n_knots\": [5, 10, 15],\n",
    "            \"polynomialfeatures\": [None, PolynomialFeatures(interaction_only=True)],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_params = {\n",
    "    \"nrows\": 5,\n",
    "    \"ncols\": 6,\n",
    "    \"figsize\": (40, 35),\n",
    "    \"sharex\": True,\n",
    "    \"sharey\": True,\n",
    "}\n",
    "boundary_figure, boundary_axes = plt.subplots(**fig_params)\n",
    "calibration_figure, calibration_axes = plt.subplots(**fig_params)\n",
    "\n",
    "for idx, (model_params, ax_boundary, ax_calibration) in enumerate(\n",
    "    zip(param_grid, boundary_axes.ravel(), calibration_axes.ravel())\n",
    "):\n",
    "    model.set_params(**model_params).fit(X_train, y_train)\n",
    "    # Create a title\n",
    "    title = f\"{model_params['splinetransformer__n_knots']} knots\"\n",
    "    title += \" with \" if model_params[\"polynomialfeatures\"] else \" without \"\n",
    "    title += \"interaction\"\n",
    "    # Display the results\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        model, X_test, ax=ax_boundary, **params\n",
    "    )\n",
    "    ax_boundary.scatter(\n",
    "        *X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolor=\"black\", alpha=0.5\n",
    "    )\n",
    "    ax_boundary.set(\n",
    "        xlim=(-3, 3),\n",
    "        ylim=(-3, 3),\n",
    "        aspect=\"equal\",\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    CalibrationDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        strategy=\"quantile\",\n",
    "        n_bins=10,\n",
    "        ax=ax_calibration,\n",
    "    )\n",
    "    ax_calibration.set(aspect=\"equal\", title=title)\n",
    "\n",
    "    if idx % fig_params[\"ncols\"] == 0:\n",
    "        for ax in (ax_boundary, ax_calibration):\n",
    "            ylabel = f\"Regularization 'C': {model_params['logisticregression__C']}\"\n",
    "            ylabel += f\"\\n\\n\\n{ax.get_ylabel()}\" if ax.get_ylabel() else \"\"\n",
    "            ax.set(ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65967267",
   "metadata": {},
   "source": [
    "\n",
    "An obvious observation is that without explicitly creating the interaction\n",
    "terms, our model is fundamentally mis-specified: model cannot represent the\n",
    "non-linear relationship, whatever the other hyperparameters values.\n",
    "\n",
    "A large enough number of knots in the spline transformation combined with\n",
    "interactions increases the flexibility of the learning procedure: the\n",
    "decision boundary can isolate more and more subregions of the feature space.\n",
    "Therefore, if we use a too large number of knots, then the model is able\n",
    "isolate noisy training data points when `C` allows.\n",
    "\n",
    "Indeed, the parameter `C` controls the loss function that is minimized during\n",
    "the training: a small value of `C` enforces to minimize the norm of the model\n",
    "coefficients and thus discard the influence of changes in feature values. A\n",
    "large value of `C` enforces to prioritize minimizing the training error\n",
    "without constraining, more or less, the norm of the coefficients.\n",
    "\n",
    "There therefore an interaction between the number of knots and the\n",
    "regularization parameter `C`: a model with a larger number of knots is more\n",
    "flexible and thus more prone to overfitting, the optimal value of the\n",
    "parameter `C` should be smaller (i.e. more regularization) than a model with\n",
    "a smaller number of knots.\n",
    "\n",
    "For instance, setting `C=100` with `n_knots=5` leads to a model with a\n",
    "similar calibration curve as setting `C=10` with `n_knots=15`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8b777",
   "metadata": {},
   "source": [
    "\n",
    "## Is it true for other models?\n",
    "\n",
    "In this section, we want to show that the previous findings are not specific\n",
    "to the a linear model that relies on a pre-processing step. Here, we use a\n",
    "gradient-boosting model that naturally captures non-linear relationships of\n",
    "the XOR problem without any need for a pre-processing step.\n",
    "\n",
    "We the impact of the choice for the `max_leaf_nodes` and `learning_rate`\n",
    "hyperparameters on the calibration curves when holding the number of boosting\n",
    "iteration fixed. Those hyperparameters are known to impact the model\n",
    "complexity and therefore the under-fitting/over-fitting trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab26483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "\n",
    "param_grid = list(\n",
    "    ParameterGrid({\"max_leaf_nodes\": [5, 10, 30], \"learning_rate\": [0.01, 0.1, 1]})\n",
    ")\n",
    "\n",
    "fig_params = {\n",
    "    \"nrows\": 3,\n",
    "    \"ncols\": 3,\n",
    "    \"figsize\": (16, 16),\n",
    "    \"sharex\": True,\n",
    "    \"sharey\": True,\n",
    "}\n",
    "boundary_figure, boundary_axes = plt.subplots(**fig_params)\n",
    "calibration_figure, calibration_axes = plt.subplots(**fig_params)\n",
    "\n",
    "for idx, (model_params, ax_boundary, ax_calibration) in enumerate(\n",
    "    zip(param_grid, boundary_axes.ravel(), calibration_axes.ravel())\n",
    "):\n",
    "    model.set_params(**model_params).fit(X_train, y_train)\n",
    "    # Create a title\n",
    "    title = f\"Maximum number of leaf nodes: {model_params['max_leaf_nodes']}\"\n",
    "    # Display the results\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        model, X_train, ax=ax_boundary, **params\n",
    "    )\n",
    "    ax_boundary.scatter(\n",
    "        *X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolor=\"black\", alpha=0.5\n",
    "    )\n",
    "    ax_boundary.set(\n",
    "        xlim=(-3, 3),\n",
    "        ylim=(-3, 3),\n",
    "        aspect=\"equal\",\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    CalibrationDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        strategy=\"quantile\",\n",
    "        n_bins=10,\n",
    "        ax=ax_calibration,\n",
    "    )\n",
    "    ax_calibration.set(aspect=\"equal\", title=title)\n",
    "\n",
    "    if idx % fig_params[\"ncols\"] == 0:\n",
    "        for ax in (ax_boundary, ax_calibration):\n",
    "            ylabel = f\"Learning rate: {model_params['learning_rate']}\"\n",
    "            ylabel += f\"\\n\\n\\n{ax.get_ylabel()}\" if ax.get_ylabel() else \"\"\n",
    "            ax.set(ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae626073",
   "metadata": {},
   "source": [
    "\n",
    "From the boundary decision plots, we observe that all the explored models are\n",
    "capable of capturing the link between the features and the target. However,\n",
    "if we look at the probability estimates, we still observe the same effect of\n",
    "under-fitting and over-fitting as for our polynomial classification pipeline.\n",
    "It also means that tuning the parameter `max_leaf_nodes` on this simplistic\n",
    "2D dataset is not worth it since for a single decision tree, the perfect\n",
    "decision boundary is achieved with only 4 leaf nodes. This would not be the\n",
    "case on more complex datasets such as a noisy checkerboard classification\n",
    "task for instance.\n",
    "\n",
    "However, the learning rate is the parameter that controls if the model\n",
    "under-fits or over-fits. A too low learning rate leads to an under-fitting\n",
    "model and the model is underconfident with probability estimates that are too\n",
    "close to 0.5, even in low ambiguity regions of the feature space. On the\n",
    "other hand, a too high learning rate leads to an over-fitting model and the\n",
    "model is over-confident with probability estimates that are too close to 0 or\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7dce2",
   "metadata": {},
   "source": [
    "\n",
    "## Calibration-aware hyperparameter tuning\n",
    "\n",
    "From the previous sections, we saw that the hyperparameters of a model while\n",
    "impacting its complexity also impact its calibration. It therefore becomes\n",
    "crucial to consider calibration when tuning the hyperparameters of a model.\n",
    "While scikit-learn offers tools to tune hyperparameters such as\n",
    "`GridSearchCV` or `RandomizedSearchCV`, there is a caveat: the default metric\n",
    "used to select the best model is not necessarily the one leading to a\n",
    "well-calibrated model.\n",
    "\n",
    "To illustrate this point, we use the previous polynomial pipeline. From the\n",
    "previous experiment, we draw the conclusion that we need to have some\n",
    "regularization to avoid overfitting when the number of knots is large enough.\n",
    "Therefore, we plot the validation curve for different values of the\n",
    "regularization parameter `C`. In addition, since we want to see the impact of\n",
    "the metric used to tuned the hyperparameters, we plot different validation\n",
    "curves for different metrics:\n",
    "- the negative log-likelihood that is a proper scoring rule,\n",
    "- the ROC AUC that is a ranking metric,\n",
    "- the accuracy that is a thresholded metric.\n",
    "\n",
    "Here we simulate 200 iterations of selecting the best value of C using the\n",
    "mean cross-validation across 5 iteration via a form of bootstrapping. The\n",
    "objective is to assess the stability of the tuning procedure for different\n",
    "choices of the classification metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "from sklearn.model_selection import ShuffleSplit, validation_curve\n",
    "\n",
    "model = make_pipeline(\n",
    "    SplineTransformer(n_knots=15),\n",
    "    PolynomialFeatures(interaction_only=True),\n",
    "    LogisticRegression(max_iter=10_000),\n",
    ")\n",
    "\n",
    "# Since the computation of the validation curve is expensive, we reuse\n",
    "# precomputed results when available on disk.\n",
    "\n",
    "n_splits, param_range = 100, np.logspace(-2, 4, 30)\n",
    "test_scores = {}\n",
    "for metric_name in [\"neg_log_loss\", \"roc_auc\", \"accuracy\"]:\n",
    "    results_file_path = Path(f\"../results/validation_curve_{metric_name}.npz\")\n",
    "    if not results_file_path.is_file():\n",
    "        _, test_scores_metric = validation_curve(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            param_name=\"logisticregression__C\",\n",
    "            param_range=param_range,\n",
    "            scoring=metric_name,\n",
    "            cv=ShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=0),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        parent_folder = results_file_path.parent\n",
    "        if not parent_folder.is_dir():\n",
    "            parent_folder.mkdir(parents=True)\n",
    "        np.savez(results_file_path, test_scores=test_scores_metric)\n",
    "        test_scores[metric_name] = test_scores_metric\n",
    "    else:\n",
    "        with np.load(results_file_path) as data:\n",
    "            test_scores[metric_name] = data[\"test_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "full_metric_name = {\n",
    "    \"neg_log_loss\": \"negative log loss\",\n",
    "    \"roc_auc\": \"ROC AUC\",\n",
    "    \"accuracy\": \"accuracy\",\n",
    "}\n",
    "for idx, (metric_name, ax) in enumerate(\n",
    "    zip([\"neg_log_loss\", \"roc_auc\", \"accuracy\"], axes)\n",
    "):\n",
    "    rng = np.random.default_rng(0)\n",
    "    bootstrap_size = 5\n",
    "    ax_hist = make_axes_locatable(ax).append_axes(\"top\", size=\"20%\", pad=0.1, sharex=ax)\n",
    "    all_best_param_values = []\n",
    "    for _ in range(200):\n",
    "        selected_fold_idx = rng.choice(n_splits, size=bootstrap_size, replace=False)\n",
    "        mean_test_score = test_scores[metric_name][:, selected_fold_idx].mean(axis=1)\n",
    "        ax.plot(\n",
    "            param_range,\n",
    "            mean_test_score,\n",
    "            color=\"tab:blue\",\n",
    "            linewidth=0.1,\n",
    "            zorder=-1,\n",
    "        )\n",
    "        best_param_idx = mean_test_score.argmax()\n",
    "        best_param_value = param_range[best_param_idx]\n",
    "        best_test_score = mean_test_score[best_param_idx]\n",
    "        ax.vlines(\n",
    "            best_param_value,\n",
    "            ymin=test_scores[metric_name].min(),\n",
    "            ymax=best_test_score,\n",
    "            linewidth=0.3,\n",
    "            color=\"tab:orange\",\n",
    "        )\n",
    "        all_best_param_values.append(best_param_value)\n",
    "    ax.set(\n",
    "        xlabel=\"Regularization C\",\n",
    "        ylabel=full_metric_name[metric_name],\n",
    "        xscale=\"log\",\n",
    "    )\n",
    "    bins = (param_range[:-1] + param_range[1:]) / 2\n",
    "    ax_hist.hist(\n",
    "        all_best_param_values, bins=bins, color=\"tab:orange\", edgecolor=\"black\"\n",
    "    )\n",
    "    ax_hist.xaxis.set_tick_params(labelleft=False, labelbottom=False)\n",
    "    ax_hist.yaxis.set_tick_params(labelleft=False, labelbottom=False)\n",
    "_ = fig.suptitle(\"Stability of parameter tuning based on different metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547dcd3c",
   "metadata": {},
   "source": [
    "\n",
    "From the previous plots, there are three important observations.\n",
    "\n",
    "First, the proper scoring rule (i.e. the negative log-likelihood) depicts a\n",
    "more distinct bump in comparison to the ranking metric (i.e. the ROC AUC) and\n",
    "the thresholded metric (i.e. the accuracy). The bump is still present for the\n",
    "ROC AUC but it is less pronounced. The accuracy does not show an a clearly\n",
    "located bump.\n",
    "\n",
    "Then, the proper scoring rule is the only one showing a significant decrease\n",
    "in model performance when the regularization is too low. The intuition is\n",
    "that the model becomes over-confident and thus not well-calibrated. The other\n",
    "metrics do not penalize overconfidence.\n",
    "\n",
    "Lastly, the proper scoring rule is the metric showing the least variability\n",
    "variability across different resampling when identifying the best\n",
    "hyperparameter. The ranking-only metric and the hard classification metric\n",
    "show a larger variability. This is due to the fact that the proper scoring\n",
    "rule is a more informative evaluation metric for probabilitic classifiers. It\n",
    "therefore makes it a more robust metric to select the best model.\n",
    "\n",
    "We therefore recommend to use a proper scoring rule when tuning the\n",
    "hyperparemeters of a probabilistic classifier. Below, we show the methodology\n",
    "to pursue when using a proper scoring together with a `RandomizedSearchCV` by\n",
    "setting `scoring` to `\"neg_log_loss\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da34e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    \"splinetransformer__n_knots\": [5, 10, 15],\n",
    "    \"logisticregression__C\": loguniform(1e-6, 1e6),\n",
    "}\n",
    "\n",
    "tuned_model = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    cv=ShuffleSplit(n_splits=10, test_size=0.2, random_state=0),\n",
    "    random_state=0,\n",
    ")\n",
    "tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a165d",
   "metadata": {},
   "source": [
    "\n",
    "Now that we trained the model, we check if it is well-calibrated on the left-out\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35385d5f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(tuned_model, X_test, ax=ax[0], **params)\n",
    "ax[0].scatter(*X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5)\n",
    "\n",
    "_ = ax[0].set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "\n",
    "CalibrationDisplay.from_estimator(\n",
    "    tuned_model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    strategy=\"quantile\",\n",
    "    n_bins=10,\n",
    "    ax=ax[1],\n",
    "    name=\"Tuned logistic regression\",\n",
    ")\n",
    "_ = ax[1].set(aspect=\"equal\")\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"Number of knots: {tuned_model.best_params_['splinetransformer__n_knots']}, \"\n",
    "    f\"Regularization 'C': {tuned_model.best_params_['logisticregression__C']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23519257",
   "metadata": {},
   "source": [
    "\n",
    "We see that our procedure leads to a well-calibrated model since we used a\n",
    "cross-validated as expected."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
