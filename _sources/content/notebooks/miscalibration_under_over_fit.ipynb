{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463ddce1",
   "metadata": {},
   "source": [
    "\n",
    "# Miscalibration due to inappropriate model hyperparameters\n",
    "\n",
    "Models complexity are controlled via their hyperparameters. Depending on their values,\n",
    "we can have models that are under-fitting or over-fitting. In this notebook, we\n",
    "investigate the relationship between models hyperparameters, model complexity, and\n",
    "their calibration.\n",
    "\n",
    "Let's start by defining our classification problem: we use the so-called XOR problem.\n",
    "The function `xor_generator` generates a dataset with two features and the target\n",
    "variable following the XOR logic. We add some noise to the generative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb87504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have scikit-learn >= 1.5\n",
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c040147",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def xor_generator(n_samples=1_000, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.uniform(low=-3, high=3, size=(n_samples, 2))\n",
    "    unobserved = rng.normal(loc=0, scale=0.5, size=(n_samples, 2))\n",
    "    y = np.logical_xor(X[:, 0] + unobserved[:, 0] > 0, X[:, 1] + unobserved[:, 1] > 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a44c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "We can now generate a dataset and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, y_train = xor_generator(seed=0)\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(*X_train.T, c=y_train, cmap=\"coolwarm\", edgecolors=\"black\", alpha=0.5)\n",
    "_ = ax.set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    title=\"XOR problem\",\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e95bc",
   "metadata": {},
   "source": [
    "\n",
    "The XOR problem exhibits a non-linear decision link between the features and the\n",
    "the target variable. Therefore, a linear model is not be able to separate the\n",
    "classes correctly. Let's confirm this intuition by fitting a logistic regression\n",
    "model to such dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e04569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dee8e",
   "metadata": {},
   "source": [
    "\n",
    "To check the decision boundary of the model, we use an independent test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "X_test, y_test = xor_generator(n_samples=10_000, seed=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "params = {\n",
    "    \"cmap\": \"coolwarm\",\n",
    "    \"response_method\": \"predict_proba\",\n",
    "    \"plot_method\": \"contourf\",\n",
    "    # make sure to have a range of 0 to 1 for the probability\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 1,\n",
    "}\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, X_test, ax=ax, **params)\n",
    "ax.scatter(*X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5)\n",
    "fig.colorbar(disp.surface_, ax=ax, label=\"Probability estimate\")\n",
    "_ = ax.set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    title=\"Soft decision boundary of a logistic regression\",\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b5f411",
   "metadata": {},
   "source": [
    "\n",
    "We see that the probability estimates is almost constant and the model is really\n",
    "uncertain with an estimated probability of 0.5 for all samples in the test set.\n",
    "\n",
    "We therefore need a more expressive model to capture the non-linear relationship\n",
    "between the features and the target variable. Crafting a pre-processing step to\n",
    "transform the features into a higher-dimensional space could help. We create a\n",
    "pipeline that includes a spline transformation and a polynomial transformation before\n",
    "to train our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36654c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    SplineTransformer(),\n",
    "    # Only add interaction terms to avoid blowing up the number of features\n",
    "    PolynomialFeatures(interaction_only=True),\n",
    "    # Increase the number of iterations to ensure convergence\n",
    "    LogisticRegression(max_iter=10_000),\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10070c4",
   "metadata": {},
   "source": [
    "\n",
    "Let's check the decision boundary of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, X_test, ax=ax, **params)\n",
    "ax.scatter(*X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5)\n",
    "fig.colorbar(disp.surface_, ax=ax, label=\"Probability estimate\")\n",
    "_ = ax.set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    title=\"Soft decision boundary of a logistic regression\\n with pre-processing\",\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e67574",
   "metadata": {},
   "source": [
    "\n",
    "We see that our model is capable of capturing the non-linear relationship between\n",
    "the features and the target variable. The probability estimates are now varying\n",
    "across the samples. We could check the calibration of our model using the calibration\n",
    "curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "disp = CalibrationDisplay.from_estimator(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    strategy=\"quantile\",\n",
    "    n_bins=10,\n",
    ")\n",
    "_ = disp.ax_.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ebe3c",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the calibration of the model is not perfect. So is there a way to\n",
    "improve the calibration of our model?\n",
    "\n",
    "As an exercise, let's try to three different hyperparameters configurations:\n",
    "- one configuration with 5 knots (i.e. `n_knots`) for the spline transformation and a\n",
    "  regularization parameter `C` of 1e-4 for the logistic regression,\n",
    "- one configuration with 7 knots for the spline transformation and a regularization\n",
    "  parameter `C` of 1e1 for the logistic regression,\n",
    "- one configuration with 15 knots for the spline transformation and a regularization\n",
    "  parameter `C` of 1e4 for the logistic regression.\n",
    "\n",
    "For each configuration, plot the decision boundary and the calibration curve. What\n",
    "can you observe in terms of under-/over-fitting and calibration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_configs = [\n",
    "    {\"splinetransformer__n_knots\": 5, \"logisticregression__C\": 1e-4},\n",
    "    {\"splinetransformer__n_knots\": 7, \"logisticregression__C\": 1e1},\n",
    "    {\"splinetransformer__n_knots\": 15, \"logisticregression__C\": 1e4},\n",
    "]\n",
    "\n",
    "for model_params in param_configs:\n",
    "    model.set_params(**model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(model, X_test, ax=ax[0], **params)\n",
    "    ax[0].scatter(\n",
    "        *X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5\n",
    "    )\n",
    "\n",
    "    ax[0].set(\n",
    "        xlim=(-3, 3),\n",
    "        ylim=(-3, 3),\n",
    "        xlabel=\"Feature 1\",\n",
    "        ylabel=\"Feature 2\",\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "\n",
    "    CalibrationDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        strategy=\"quantile\",\n",
    "        n_bins=10,\n",
    "        ax=ax[1],\n",
    "    )\n",
    "    ax[1].set(aspect=\"equal\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Number of knots: {model_params['splinetransformer__n_knots']}, \"\n",
    "        f\"Regularization 'C': {model_params['logisticregression__C']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba47863",
   "metadata": {},
   "source": [
    "\n",
    "From the previous exercise, we observe that whether we have an under-fitting or\n",
    "over-fitting model impact its calibration. With a high regularization (i.e. `C=1e-4`),\n",
    "we see that the model undefits since it does not discriminate between the two classes.\n",
    "It translates into obtaining a vertical calibration curve meaning that our model\n",
    "predicts the same probability for all fraction of positive samples.\n",
    "\n",
    "On the other hand, if we have a low regularization (i.e. `C=1e4`), and allows the\n",
    "the model to be flexible by having a large number of knots, we see that the model\n",
    "overfits since it is able to isolate noisy samples in the feature space. It translates\n",
    "into a calibration curve where we observe that our model is over-confident.\n",
    "\n",
    "Finally, there is a sweet spot where the model does not underfit nor overfit. In this\n",
    "case, we also get a calibrated model.\n",
    "\n",
    "We can push the analysis further by looking at a wider range of hyperparameters:\n",
    "\n",
    "- the impact of `n_knots` of the `SplineTransformer`,\n",
    "- whether or not to compute interaction terms using a `PolynomialFeatures`,\n",
    "- the impact of the parameter `C` of the `LogisticRegression`.\n",
    "\n",
    "We can plot the full grid of hyperparameters to see the effect on the decision\n",
    "boundary and the calibration curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = list(\n",
    "    ParameterGrid(\n",
    "        {\n",
    "            \"logisticregression__C\": np.logspace(-1, 3, 5),\n",
    "            \"splinetransformer__n_knots\": [5, 10, 15],\n",
    "            \"polynomialfeatures\": [None, PolynomialFeatures(interaction_only=True)],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_params = {\n",
    "    \"nrows\": 5,\n",
    "    \"ncols\": 6,\n",
    "    \"figsize\": (40, 35),\n",
    "    \"sharex\": True,\n",
    "    \"sharey\": True,\n",
    "}\n",
    "boundary_figure, boundary_axes = plt.subplots(**fig_params)\n",
    "calibration_figure, calibration_axes = plt.subplots(**fig_params)\n",
    "\n",
    "for idx, (model_params, ax_boundary, ax_calibration) in enumerate(\n",
    "    zip(param_grid, boundary_axes.ravel(), calibration_axes.ravel())\n",
    "):\n",
    "    model.set_params(**model_params).fit(X_train, y_train)\n",
    "    # Create a title\n",
    "    title = f\"{model_params['splinetransformer__n_knots']} knots\"\n",
    "    title += \" with \" if model_params[\"polynomialfeatures\"] else \" without \"\n",
    "    title += \"interaction terms\"\n",
    "    # Display the results\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        model, X_test, ax=ax_boundary, **params\n",
    "    )\n",
    "    ax_boundary.scatter(\n",
    "        *X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolor=\"black\", alpha=0.5\n",
    "    )\n",
    "    ax_boundary.set(\n",
    "        xlim=(-3, 3),\n",
    "        ylim=(-3, 3),\n",
    "        aspect=\"equal\",\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    CalibrationDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        strategy=\"quantile\",\n",
    "        n_bins=10,\n",
    "        ax=ax_calibration,\n",
    "    )\n",
    "    ax_calibration.set(aspect=\"equal\", title=title)\n",
    "\n",
    "    if idx % fig_params[\"ncols\"] == 0:\n",
    "        for ax in (ax_boundary, ax_calibration):\n",
    "            ylabel = f\"Regularization 'C': {model_params['logisticregression__C']}\"\n",
    "            ylabel += f\"\\n\\n\\n{ax.get_ylabel()}\" if ax.get_ylabel() else \"\"\n",
    "            ax.set(ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6571ed1",
   "metadata": {},
   "source": [
    "\n",
    "An obvious observation is that without explicitly creating the interaction terms,\n",
    "our model is mis-specified and the model cannot capture the non-linear relationship,\n",
    "whatever the other hyperparameters values.\n",
    "\n",
    "A larger number of knots in the spline transformation increases the flexibility of the\n",
    "decision boundary since it can vary at more locations into the feature space.\n",
    "Therefore, if we use a too large number of knots, then the model is able isolate noisy\n",
    "samples in this feature space, depending of the subsequent regularization parameter\n",
    "`C`.\n",
    "\n",
    "Indeed, the parameter `C` controls the loss function that is minimized during the\n",
    "training: a small value of `C` enforces to minimize the norm of the model coefficients\n",
    "and thus discard, more or less, the training error (i.e. the mean squared error). A\n",
    "large value of `C` enforces to prioritize minimizing the training error without\n",
    "constraining, more or less, the norm of the coefficients.\n",
    "\n",
    "Understanding the previous principles, it allows us to understand that we have an\n",
    "interaction between the number of knots and the regularization parameter `C`. Since a\n",
    "model with a larger number of knots is more flexible and thus more prone to\n",
    "overfitting, the value of the parameter `C` should be smaller (i.e. more\n",
    "regularization) than a model with a smaller number of knots.\n",
    "\n",
    "For instance, setting `C=100` with `n_knots=5` leads to a model with a similar\n",
    "calibration curve as setting `C=10` with `n_knots=15`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f97fc7",
   "metadata": {},
   "source": [
    "\n",
    "## Is it true for other models?\n",
    "\n",
    "In this section, we want to show that the previous findings are not specific to the\n",
    "a linear model that relies on a pre-processing step. Here, we use a gradient-boosting\n",
    "model that naturally captures non-linear relationships of the XOR problem.\n",
    "\n",
    "We check that the calibration of the model by changing the hyperparameters\n",
    "`max_leaf_nodes` and `learning_rate` that are known to impact the model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "\n",
    "param_grid = list(\n",
    "    ParameterGrid({\"max_leaf_nodes\": [5, 10, 30], \"learning_rate\": [0.01, 0.1, 1]})\n",
    ")\n",
    "\n",
    "fig_params = {\n",
    "    \"nrows\": 3,\n",
    "    \"ncols\": 3,\n",
    "    \"figsize\": (16, 16),\n",
    "    \"sharex\": True,\n",
    "    \"sharey\": True,\n",
    "}\n",
    "boundary_figure, boundary_axes = plt.subplots(**fig_params)\n",
    "calibration_figure, calibration_axes = plt.subplots(**fig_params)\n",
    "\n",
    "for idx, (model_params, ax_boundary, ax_calibration) in enumerate(\n",
    "    zip(param_grid, boundary_axes.ravel(), calibration_axes.ravel())\n",
    "):\n",
    "    model.set_params(**model_params).fit(X_train, y_train)\n",
    "    # Create a title\n",
    "    title = f\"Maximum number of leaf nodes: {model_params['max_leaf_nodes']}\"\n",
    "    # Display the results\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        model, X_test, ax=ax_boundary, **params\n",
    "    )\n",
    "    ax_boundary.scatter(\n",
    "        *X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolor=\"black\", alpha=0.5\n",
    "    )\n",
    "    ax_boundary.set(\n",
    "        xlim=(-3, 3),\n",
    "        ylim=(-3, 3),\n",
    "        aspect=\"equal\",\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    CalibrationDisplay.from_estimator(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        strategy=\"quantile\",\n",
    "        n_bins=10,\n",
    "        ax=ax_calibration,\n",
    "    )\n",
    "    ax_calibration.set(aspect=\"equal\", title=title)\n",
    "\n",
    "    if idx % fig_params[\"ncols\"] == 0:\n",
    "        for ax in (ax_boundary, ax_calibration):\n",
    "            ylabel = f\"Learning rate: {model_params['learning_rate']}\"\n",
    "            ylabel += f\"\\n\\n\\n{ax.get_ylabel()}\" if ax.get_ylabel() else \"\"\n",
    "            ax.set(ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365f77",
   "metadata": {},
   "source": [
    "\n",
    "From the boundary decision plots, we observe that the model, whatever the\n",
    "hyperparameters, is capable of capturing the link between the features and the target.\n",
    "However, if we look at the probability estimates, we still observe the same effect of\n",
    "under-fitting and over-fitting as for the logistic regression model. It also means\n",
    "that tuning the parameter `max_leaf_nodes` on this specific dataset is not worth it\n",
    "since for a single decision tree, the perfect decision boundary is achieved with\n",
    "4 leaf nodes.\n",
    "\n",
    "However, the learning rate is the parameter that controls the model to under-fit or\n",
    "over-fit. A too low learning rate leads to an under-fitting model and the model is\n",
    "under-confident with probability estimates that are too low. On the other hand, a too\n",
    "high learning rate leads to an over-fitting model and the model is over-confident with\n",
    "probability estimates that are too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95243e",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter tuning while considering calibration\n",
    "\n",
    "From the previous sections, we saw that the hyperparameters of a model while impacting\n",
    "its complexity also impact its calibration. It therefore becomes crucial to tune the\n",
    "hyperparameters of a model while considering if its calibration. While scikit-learn\n",
    "offers tools to tune hyperparameters such as `GridSearchCV` or `RandomizedSearchCV`,\n",
    "there is a caveat: the default metric used to select the best model is not necessarily\n",
    "the one leading to a well-calibrated model.\n",
    "\n",
    "To illustrate this point, we use the previous logistic regression model with the\n",
    "preprocessing step. From the previous experiment, we draw the conclusion that we\n",
    "need to have some regularization to avoid overfitting induced by the number of knots.\n",
    "Therefore, we plot the validate curve for different values of the regularization\n",
    "parameter `C`. In addition, since we want to see the impact of the metric used to\n",
    "tuned the hyperparameters, we plot different validation curves for different metrics:\n",
    "- the negative log-likelihood that is a proper scoring rule,\n",
    "- the ROC AUC that is a ranking metric,\n",
    "- the accuracy that is a thresholded metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d977034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "from sklearn.model_selection import ShuffleSplit, validation_curve\n",
    "\n",
    "model = make_pipeline(\n",
    "    SplineTransformer(n_knots=15),\n",
    "    PolynomialFeatures(interaction_only=True),\n",
    "    LogisticRegression(max_iter=10_000),\n",
    ")\n",
    "\n",
    "# Since the computation of the validation curve is expensive, we stored the results\n",
    "# and commit them in the repository. If the folder containing the results does not\n",
    "# exist, we compute the validation curve and store the results.\n",
    "\n",
    "n_splits, param_range = 100, np.logspace(-2, 4, 30)\n",
    "test_scores = {}\n",
    "for metric_name in [\"neg_log_loss\", \"roc_auc\", \"accuracy\"]:\n",
    "    results_file_path = Path(f\"../results/validation_curve_{metric_name}.npz\")\n",
    "    if not results_file_path.is_file():\n",
    "        _, test_scores_metric = validation_curve(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            param_name=\"logisticregression__C\",\n",
    "            param_range=param_range,\n",
    "            scoring=metric_name,\n",
    "            cv=ShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=0),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        parent_folder = results_file_path.parent\n",
    "        if not parent_folder.is_dir():\n",
    "            parent_folder.mkdir(parents=True)\n",
    "        np.savez(results_file_path, test_scores=test_scores_metric)\n",
    "        test_scores[metric_name] = test_scores_metric\n",
    "    else:\n",
    "        with np.load(results_file_path) as data:\n",
    "            test_scores[metric_name] = data[\"test_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128aa0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "full_metric_name = {\n",
    "    \"neg_log_loss\": \"negative log loss\",\n",
    "    \"roc_auc\": \"ROC AUC\",\n",
    "    \"accuracy\": \"accuracy\",\n",
    "}\n",
    "for idx, (metric_name, ax) in enumerate(\n",
    "    zip([\"neg_log_loss\", \"roc_auc\", \"accuracy\"], axes)\n",
    "):\n",
    "    rng = np.random.default_rng(0)\n",
    "    bootstrap_size = 5\n",
    "    ax_hist = make_axes_locatable(ax).append_axes(\"top\", size=\"20%\", pad=0.1, sharex=ax)\n",
    "    all_best_param_values = []\n",
    "    for _ in range(200):\n",
    "        selected_fold_idx = rng.choice(n_splits, size=bootstrap_size, replace=False)\n",
    "        mean_test_score = test_scores[metric_name][:, selected_fold_idx].mean(axis=1)\n",
    "        ax.plot(\n",
    "            param_range,\n",
    "            mean_test_score,\n",
    "            color=\"tab:blue\",\n",
    "            linewidth=0.1,\n",
    "            zorder=-1,\n",
    "        )\n",
    "        best_param_idx = mean_test_score.argmax()\n",
    "        best_param_value = param_range[best_param_idx]\n",
    "        best_test_score = mean_test_score[best_param_idx]\n",
    "        ax.vlines(\n",
    "            best_param_value,\n",
    "            ymin=test_scores[metric_name].min(),\n",
    "            ymax=best_test_score,\n",
    "            linewidth=0.3,\n",
    "            color=\"tab:orange\",\n",
    "        )\n",
    "        all_best_param_values.append(best_param_value)\n",
    "    ax.set(\n",
    "        xlabel=\"Regularization C\",\n",
    "        ylabel=full_metric_name[metric_name],\n",
    "        xscale=\"log\",\n",
    "    )\n",
    "    bins = (param_range[:-1] + param_range[1:]) / 2\n",
    "    ax_hist.hist(\n",
    "        all_best_param_values, bins=bins, color=\"tab:orange\", edgecolor=\"black\"\n",
    "    )\n",
    "    ax_hist.xaxis.set_tick_params(labelleft=False, labelbottom=False)\n",
    "    ax_hist.yaxis.set_tick_params(labelleft=False, labelbottom=False)\n",
    "_ = fig.suptitle(\"Stability of parameter tuning based on different metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99286a0b",
   "metadata": {},
   "source": [
    "\n",
    "From the previous plots, there are three important observations.\n",
    "\n",
    "First, the proper scoring rule (i.e. the negative log-likelihood) depicts a more\n",
    "distinct bump in comparison to the ranking metric (i.e. the ROC AUC) and the\n",
    "thresholded metric (i.e. the accuracy). The bump is still present for the ROC AUC but\n",
    "it is less pronounced. The accuracy does not show any bump.\n",
    "\n",
    "Then, the proper scoring rule is the only one showing a significant decrease when\n",
    "the regularization is too low. The intuition is that the model becomes over-confident\n",
    "and thus not well-calibrated while the hard predictions are not be impacted.\n",
    "\n",
    "Lastly, the proper scoring rule is the metric showing the least variance across the\n",
    "different splits near of the optimal value. It therefore makes it a more robust metric\n",
    "to select the best model.\n",
    "\n",
    "We therefore recommend to always use a proper scoring rule when tuning the\n",
    "hyperparemeters. Below, we show the methodology to pursue when using a proper scoring\n",
    "together with a `RandomizedSearchCV`. We therefore needs to set specifically\n",
    "`scoring` to `neg_log_loss` in the `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e701e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    \"splinetransformer__n_knots\": [5, 10, 15],\n",
    "    \"logisticregression__C\": loguniform(1e-6, 1e6),\n",
    "}\n",
    "\n",
    "tuned_model = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25,\n",
    "    scoring=\"neg_log_loss\",\n",
    "    cv=ShuffleSplit(n_splits=10, test_size=0.2, random_state=0),\n",
    "    random_state=0,\n",
    ")\n",
    "tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033853f8",
   "metadata": {},
   "source": [
    "\n",
    "Now that we trained the model, we check if it is well-calibrated on the left-out\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacb5fb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(tuned_model, X_test, ax=ax[0], **params)\n",
    "ax[0].scatter(*X_train.T, c=y_train, cmap=params[\"cmap\"], edgecolors=\"black\", alpha=0.5)\n",
    "\n",
    "_ = ax[0].set(\n",
    "    xlim=(-3, 3),\n",
    "    ylim=(-3, 3),\n",
    "    xlabel=\"Feature 1\",\n",
    "    ylabel=\"Feature 2\",\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "\n",
    "CalibrationDisplay.from_estimator(\n",
    "    tuned_model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    strategy=\"quantile\",\n",
    "    n_bins=10,\n",
    "    ax=ax[1],\n",
    "    name=\"Tuned logistic regression\",\n",
    ")\n",
    "_ = ax[1].set(aspect=\"equal\")\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"Number of knots: {tuned_model.best_params_['splinetransformer__n_knots']}, \"\n",
    "    f\"Regularization 'C': {tuned_model.best_params_['logisticregression__C']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e4cb6",
   "metadata": {},
   "source": [
    "\n",
    "We see that our procedure leads to a well-calibrated model since we used a\n",
    "cross-validated proper scoring rule to select the best hyper-parameter\n",
    "combination."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
